# BM25 A Non-Learning Baseline for Information Retrieval
[View on Reference Site](https://www.nowpublishers.com/article/Details/INR-019)

## ğŸ“„ Paper 18: â€œBM25: A Non-Learning Baseline for Information Retrievalâ€
ğŸ“œ Originally described in Robertson et al., 1995â€“2009  
**BM25 = Best Matching 25**

### ğŸ” Simple Explanation:
**BM25** is a **classic, non-ML algorithm** used for **ranking documents based on relevance** to a search query. Despite being decades old, itâ€™s still a strong baseline for modern retrieval systemsâ€”and often used as a benchmark against newer neural methods like vector embeddings.

Itâ€™s based on how often query words appear in documents, and how common those words are overall.

### ğŸ§  Key Concepts:
- **TF (Term Frequency)**: How often a word shows up in a document.
- **IDF (Inverse Document Frequency)**: How rare a word is across all documents.
- **BM25 Formula**: Scores documents using a clever combination of TF and IDF, plus length normalization.

### ğŸ§® Scoring Intuition:
- Frequent query terms in a document = higher score
- Common terms across many docs (like â€œtheâ€) are downweighted
- Long documents donâ€™t get unfairly rewarded

### ğŸ’¡ Real-World Analogy:
Itâ€™s like Google search in the 1990s: matching based on keyword frequency and adjusting for how common the words are. Surprisingly, it still holds up remarkably wellâ€”even compared to fancy neural models.

### ğŸ§© Why It Matters:
- **Extremely fast and simple to implement**
- Still used in modern search engines as a **fallback or hybrid** with vector search
- Commonly paired with embedding models for **re-ranking or fusion methods**
- Forms the **baseline comparison in most retrieval benchmarks**, including RAG and LangChain apps