# Language Models are Few-Shot Learners

Language Models are Few-Shot Learners

Link to paper: https://arxiv.org/abs/2005.14165


## 📄 Paper 2: “Language Models are Few-Shot Learners”
**Authors**: Brown et al., 2020

### 🔍 Simple Explanation:
This paper introduces **GPT-3**, one of the first truly massive language models trained on a huge dataset. The surprising discovery? **You don’t need to fine-tune it**—you can just **give it a few examples** of what you want, and it figures out the task.

This ability is called **few-shot learning**, and it makes the model extremely flexible.

### 🧠 Key Concepts:
- **Few-Shot Learning**: You show the model 2–3 examples in your prompt, and it generalizes the task.
- **Zero-Shot Learning**: The model does tasks without *any* examples, just based on a well-phrased instruction.
- **In-Context Learning**: Rather than retraining the model, you feed it the context it needs in the prompt itself.

### 📊 What Makes GPT-3 Special:
- **175 billion parameters** – a huge leap from previous models.
- Trained to simply predict the next word in a sentence, but ends up learning everything from translation to programming patterns.

### 💡 Real-World Analogy:
Imagine hiring someone who’s never been trained for your specific job but learns everything they need just by looking at a few examples you scribble on a napkin. That’s GPT-3.

### 🧩 Why It Matters:
This paper is the backbone of the modern LLM era:
- It proved that **scale alone** could produce general intelligence.
- Enabled tools like ChatGPT, Copilot, and Claude to perform tasks with minimal instruction.