# Contextual Retrieval for Large Language Models
[View on Anthropic](https://www.anthropic.com/news/contextual-retrieval)

## ğŸ“„ Paper 20: â€œContextual Retrieval for Large Language Modelsâ€
**Authors**: Anthropic Team, 2023  
**AKA**: The *â€œClaude gets smart about what to readâ€* technique

### ğŸ” Simple Explanation:
This paper describes **how to make retrieval smarter** by focusing not just on keyword matching or embeddingsâ€”but on **contextual relevance**. That means dynamically selecting the **most useful** passages for a specific question, using the **model itself to guide retrieval**.

### ğŸ§  Key Concepts:
- **Context-Aware Retrieval**: Tailor results based on where in the conversation the question appears.
- **Semantic Relevance over Similarity**: Move beyond vector closeness to actual task relevance.
- **Model-in-the-Loop Filtering**: Use the LLM to score and filter whatâ€™s worth reading.

### ğŸ“Š Key Insight:
Embedding similarity is helpful, but not enoughâ€”especially in complex or multi-turn tasks. Use the **LLMâ€™s judgment** to decide which results will help answer a question well.

### ğŸ’¡ Real-World Analogy:
Itâ€™s like a librarian who doesnâ€™t just pull books that â€œmention your topic,â€ but understands your actual question and finds **exactly the parts that help you answer it**.

### ğŸ§© Why It Matters:
- Makes RAG **more accurate, less noisy**
- Enables models to be **more grounded and trustworthy**
- Core technique behind Claude 2â€™s ability to work with **long documents and chat history**