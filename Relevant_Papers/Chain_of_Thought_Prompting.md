# Chain of Thought Prompting

Chain of Thought Prompting

Link to paper: https://arxiv.org/abs/2201.11903


## ğŸ“„ Paper 3: â€œChain of Thought Prompting Elicits Reasoning in Large Language Modelsâ€
**Authors**: Wei et al., 2022

### ğŸ” Simple Explanation:
This paper shows that large language models can solve **complex reasoning problems** much better if you ask them to **think step-by-step**. Thatâ€™s the whole idea behind **Chain of Thought (CoT) prompting**.

Instead of jumping to an answer, the model is nudged to **explain its reasoning** along the wayâ€”just like how youâ€™d walk through a math problem on paper.

### ğŸ§  Key Concepts:
- **CoT Prompting**: Ask the model to â€œthink out loudâ€ in steps.
- **Better for Complex Tasks**: Especially useful for math, logic, and multi-part questions.
- **Only Works in Big Models**: Smaller models don't benefit muchâ€”scale is key.

### âœï¸ Example:
**Question**: If there are 5 apples and you eat 2, how many are left?  
**Standard Prompt**: "Answer: 3"  
**CoT Prompt**: "There are 5 apples. I eat 2. 5 minus 2 is 3. Answer: 3"

### ğŸ’¡ Real-World Analogy:
Imagine asking a kid to do math in their head versus showing their work on paper. The second method helps you catch errors and builds better habits. Thatâ€™s what CoT does for LLMs.

### ğŸ§© Why It Matters:
- Boosts performance in tasks that require **reasoning, not just pattern matching**
- Became foundational in **agent systems** and **tool use** where reasoning steps matter