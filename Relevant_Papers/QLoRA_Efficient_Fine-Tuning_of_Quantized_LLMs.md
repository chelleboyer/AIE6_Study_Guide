# QLoRA Efficient Fine-Tuning of Quantized LLMs

QLoRA: Efficient Fine-Tuning of Quantized LLMs

Link to paper: https://arxiv.org/abs/2305.14314


## ğŸ“„ Paper 8: â€œQLoRA: Efficient Fine-Tuning of Quantized LLMsâ€
**Authors**: Dettmers et al., 2023

### ğŸ” Simple Explanation:
QLoRA makes it possible to fine-tune **massive models on small GPUs** by combining **quantization** and **LoRA**.

### ğŸ§  Key Concepts:
- **Quantization**: Shrinking model weights to use less memory
- **LoRA + Quantization**: Train adapters on a smaller, compressed model
- **Page Migration**: Smart memory handling across CPU and GPU

### ğŸ’¡ Real-World Analogy:
Itâ€™s like shrinking a giant textbook and writing notes in the marginsâ€”customized, fast, and portable.

### ğŸ§© Why It Matters:
- Fine-tune 13B+ models on consumer-grade GPUs
- Pushed LLM fine-tuning into the mainstream