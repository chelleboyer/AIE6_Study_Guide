# DeepSeekMath Pretraining LLMs for Mathematical Reasoning
[View on arXiv](https://arxiv.org/abs/2402.03300)

## ğŸ“„ Paper 13: â€œDeepSeekMath: Pretraining LLMs for Mathematical Reasoningâ€
**Authors**: DeepSeek Team, 2024  
**Notable for**: Introducing **GRPO** â€” Guided Reward Preference Optimization

### ğŸ” Simple Explanation:
This paper focuses on improving a modelâ€™s **math reasoning skills** by training it on a massive, high-quality math datasetâ€”**DeepSeekMath**. It also introduces a new training method called **GRPO**, which fine-tunes models using a more structured reward signal than traditional RLHF.

### ğŸ§  Key Concepts:
- **DeepSeekMath Dataset**: A large collection of math problems across a wide range of difficulty, designed for LLM pretraining and fine-tuning.
- **GRPO (Guided Reward Preference Optimization)**:
  - Improves alignment via *multi-turn, structured feedback*.
  - Outperforms RLHF on structured tasks like math.
- **Reasoning Over Memorization**: The model learns patterns and problem-solving processes, not just answers.

### âœï¸ Example Use Case:
Train a model to not just say â€œthe derivative of xÂ² is 2x,â€ but to explain **why** and **how** it got that result.

### ğŸ’¡ Real-World Analogy:
Itâ€™s like giving a student not just the textbook and test answers, but also an experienced tutor who gives **step-by-step feedback** based on their reasoning style.

### ğŸ§© Why It Matters:
- Pushes math-capable models closer to **human-level explanation and reasoning**  
- Introduces GRPO, an innovative method now influencing **reasoning-aligned fine-tuning**  
- Inspires new directions in LLM alignment using structured preference feedback